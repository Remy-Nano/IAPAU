<<<<<<< HEAD
import { Mistral } from "@mistralai/mistralai";
import { OpenAI } from "openai";
import { ChatCompletionMessageParam } from "openai/resources";
import { config } from "./config";
import { convertRoleForAI } from "./utils/messageUtils";

// Initialisation des clients
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const mistral = new Mistral({
  apiKey: process.env.MISTRAL_API_KEY || "",
});

/**
 * G√©n√®re une r√©ponse IA en fonction du mod√®le sp√©cifi√©
 */
export async function generateAIResponse(
  prompt: string,
  modelName: string,
  history: { role: string; content: string }[] = [],
  maxTokens: number = 512
): Promise<{ content: string; tokenCount?: number }> {
  try {
    if (process.env.E2E_TESTING === "true") {
      return {
        content: "Bonjour (mock E2E).",
        tokenCount: 0,
      };
    }

    // G√©n√©rer une r√©ponse avec OpenAI
    if (modelName.toLowerCase().includes("openai") || modelName === "gpt") {
      // Convertir les messages pour le format OpenAI
      const messages: ChatCompletionMessageParam[] = history.map((msg) => ({
        role: convertRoleForAI(msg.role),
        content: msg.content,
      }));

      // Ajouter le prompt actuel
      messages.push({ role: "user", content: prompt });

      const response = await openai.chat.completions.create({
        model: config.models.openai.defaultModel,
        messages,
        temperature: 0.7,
        max_tokens: maxTokens,
      });

      return {
        content:
          response.choices[0].message.content || "Pas de r√©ponse g√©n√©r√©e",
        tokenCount: response.usage?.total_tokens,
      };
    }

    // G√©n√©rer une r√©ponse avec Mistral
    else if (modelName.toLowerCase().includes("mistral")) {
      try {
        // Convertir les messages au format Mistral
        // Note: Mistral est plus strict sur les types, on passe donc par une approche diff√©rente
        const formattedMessages = [];

        // Ajouter les messages d'historique
        for (const msg of history) {
          formattedMessages.push({
            role: convertRoleForAI(msg.role),
            content: msg.content,
          });
        }

        // Ajouter le prompt actuel
        formattedMessages.push({ role: "user", content: prompt });

        // Appel √† l'API Mistral
        const response = await mistral.chat.complete({
          model: config.models.mistral.defaultModel,
          messages: formattedMessages as any, // Forcer le type pour contourner les limitations
          // Passer directement l'objet avec les param√®tres √† l'API
          ...{ maxTokens }, // Utiliser la syntaxe d'extension pour ajouter maxTokens
        });

        // V√©rifier si la r√©ponse est valide
        if (!response.choices || response.choices.length === 0) {
          throw new Error("Pas de r√©ponse g√©n√©r√©e par Mistral");
        }

        return {
          content: String(
            response.choices[0].message.content || "Pas de r√©ponse g√©n√©r√©e"
          ),
          tokenCount: response.usage?.totalTokens,
        };
      } catch (error) {
        console.error("Erreur sp√©cifique Mistral:", error);
        throw error;
      }
    }

    // Mod√®le non reconnu ou non support√©
    else {
      throw new Error(`Mod√®le non support√©: ${modelName}`);
    }
  } catch (error) {
    console.error(`Erreur lors de la g√©n√©ration de r√©ponse IA:`, error);
    throw new Error(
      `Erreur avec l'API ${modelName}: ${
        error instanceof Error ? error.message : String(error)
      }`
    );
  }
=======
// src/lib/ai-service.ts
import { Mistral } from "@mistralai/mistralai";
import { convertRoleForAI } from "./utils/messageUtils";

const mistralApiKey = process.env.MISTRAL_API_KEY || "";

if (!mistralApiKey) {
  console.warn("‚ö†Ô∏è MISTRAL_API_KEY non d√©finie dans l'environnement (.env.local)");
}

const mistral = new Mistral({
  apiKey: mistralApiKey,
});

// ‚úÖ Liste de mod√®les Mistral qu'on va tester dans l'ordre
const MISTRAL_MODELS = [
  "mistral-small-latest",
  "mistral-tiny-latest",
  "open-mistral-7b",
];

type HistoryMessage = { role: string; content: string };

export async function generateAIResponse(
  prompt: string,
  _modelName: string, // on ignore pour l'instant, on utilise MISTRAL_MODELS
  history: HistoryMessage[] = [],
  maxTokens: number = 512
): Promise<{ content: string; tokenCount?: number; modelUsed?: string }> {
  // On construit les messages communs
  const formattedMessages = [
    ...history.map((msg) => ({
      role: convertRoleForAI(msg.role),
      content: msg.content,
    })),
    { role: "user", content: prompt },
  ];

  let lastError: unknown = null;

  for (const model of MISTRAL_MODELS) {
    try {
      console.log(`üîÆ Tentative avec le mod√®le Mistral: ${model}`);

      const response = await mistral.chat.complete({
        model,
        messages: formattedMessages as any,
        max_tokens: maxTokens,
        temperature: 0.7,
      } as any);

      if (!response.choices || response.choices.length === 0) {
        throw new Error(`Pas de r√©ponse g√©n√©r√©e par le mod√®le ${model}`);
      }

      const content =
        (response.choices[0] as any).message?.content ||
        "Pas de r√©ponse g√©n√©r√©e";

      console.log(`‚úÖ R√©ponse obtenue avec ${model}`);

      return {
        content: String(content),
        tokenCount: (response.usage as any)?.totalTokens,
        modelUsed: model,
      };
    } catch (error: any) {
      console.error(`‚ùå Erreur avec le mod√®le ${model}:`, error);

      // On garde l'erreur pour √©ventuellement la remonter
      lastError = error;

      // Si c'est une erreur de capacit√© / 429 ‚Üí on essaie le mod√®le suivant
      const isCapacityError =
        error &&
        typeof error === "object" &&
        "statusCode" in error &&
        error.statusCode === 429;

      if (isCapacityError) {
        console.warn(
          `‚ö†Ô∏è Mod√®le ${model} satur√© (429 / capacity exceeded). On tente le mod√®le suivant‚Ä¶`
        );
        continue; // on passe au mod√®le suivant
      }

      // Si c'est une autre erreur, on ne force pas le fallback, on remonte directement
      throw new Error(
        `Erreur avec le mod√®le Mistral (${model}): ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }
  }

  // Si on arrive ici, tous les mod√®les Mistral ont √©chou√©
  throw new Error(
    `Tous les mod√®les Mistral ont √©chou√©. Derni√®re erreur: ${
      lastError instanceof Error ? lastError.message : String(lastError)
    }`
  );
>>>>>>> 51b2420 (feat: examiner evaluation flow + student results page)
}
